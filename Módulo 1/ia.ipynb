{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introdução\n",
    "\n",
    "O principal objetivo deste trabalho era reduzir o tempo necessário para encontrar a frequência de fase de sistemas binários em comparação com o método PDM. O método PDM é altamente eficiente na determinação da frequência de fase e é mais flexível em relação aos dados de entrada, em comparação com outros métodos. No entanto, quando não se tem uma ideia clara de por onde começar a buscar a frequência de fase, esse método pode se tornar computacionalmente custoso.\n",
    "\n",
    "Inicialmente, a ideia era treinar um algoritmo de regressão para aprender a identificar a relação entre a série temporal do brilho do sistema binário e sua frequência de fase. Entretanto, encontrei alguns desafios ao longo do caminho.\n",
    "Diversidade dos dados:\n",
    "\n",
    "Uma das razões para o uso do método PDM é a sua flexibilidade em relação à série de entrada. A Transformada de Fourier, por exemplo, não permite uma série com variação no intervalo de tempo entre as medidas.\n",
    "\n",
    "No contexto do algoritmo de regressão, a diversidade dos dados também se tornou um desafio, mas consegui superá-lo utilizando processamento dos dados.\n",
    "\n",
    "Uma alternativa para evitar o processamento extensivo dos dados seria o uso de redes neurais. No entanto, esses modelos são consideravelmente mais complexos de implementar.\n",
    "\n",
    "#### Poder computacional:\n",
    "\n",
    "É irônico que o principal desafio tenha sido exatamente o que eu desejava resolver. A questão é que a relação entre a frequência de fase e a série temporal do brilho não parece ser fácil de correlacionar. Isso, somado ao meu modesto notebook, que limitou o tamanho do dataset que eu pude gerar, tornou a tarefa de treinar o algoritmo de regressão bastante complicada.\n",
    "\n",
    "No entanto, é importante destacar que, embora o treinamento do algoritmo de regressão seja demorado, seu valor reside no fato de que, uma vez treinado, ele poderia reduzir drasticamente o tempo necessário para encontrar a frequência de fase de séries temporais, quase a zero.\n",
    "\n",
    "#### Resultados obtidos\n",
    "\n",
    "Apesar de não ter sido possível obter resultados satisfatórios com o uso de algoritmos de regressão, o objetivo inicial do trabalho foi alcançado. Parte do processamento de dados necessário para preparar os dados para o treinamento das IAs envolveu a interpolationolação dos dados. No entanto, ao interpolationolar os dados, conseguimos remover a variação no intervalo de tempo entre as medidas, tornando possível o uso da série de Fourier para calcular a frequência de fase.\n",
    "\n",
    "Ao utilizar esse método para calcular a frequência, obtive resultados bastante satisfatórios. A média do erro ficou próxima de 1%, variando de acordo com o dataset gerado, mesmo considerando a presença de ruído no sinal. Além disso, em termos de tempo de execução, foi possível reduzir significativamente o tempo necessário em comparação com o método PDM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal(amplitude, w0, size, resolution):\n",
    "    \"\"\" generate a sinusoidal function\n",
    "\n",
    "    Args:\n",
    "        amplitude (float): signal amplitude\n",
    "        w0 (_type_): frequency\n",
    "        size (_type_): series size\n",
    "        resolution (_type_): measurement resolution\n",
    "\n",
    "    Returns:\n",
    "        list: points of the sinusoidal function\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.round((np.sort(np.random.uniform(0, 20*np.pi, size))), resolution)\n",
    "    \n",
    "    y = amplitude*np.sin(w0*x) + np.random.normal(0, 0.1, len(x))\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(x, y, resolution):\n",
    "    \"\"\" interpolates the y points to fill in the gaps\n",
    "\n",
    "    Args:\n",
    "        x (list): x axis points of the sinusoidal function\n",
    "        y (list): y axis points of the sinusoidal function\n",
    "\n",
    "    Returns:\n",
    "        list: new interpolated y\n",
    "    \"\"\"\n",
    "\n",
    "    min_value = np.min(x)\n",
    "    max_value= np.max(x)\n",
    "\n",
    "    # Creates a new list at regular intervals\n",
    "    x_novo = np.arange(min_value, max_value, 1/10**resolution)\n",
    "\n",
    "    # Use interpolation to find the news values of y\n",
    "    interp_y = np.round(np.interp(x_novo, x, y),resolution)\n",
    "    \n",
    "    return interp_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Aqui você irá poder alterar os parâmetros para gerar o dataset. Caso queira ver a diferença entre o tempo de execução para calcular a frequência de fase\n",
    "usando a série de Fourier com os dados interpolados e o método PDM, sugiro deixar os parâmetros como estão. \n",
    "\n",
    "Para treinar os algoritmos de machine learning, sugiro reduzir a resolução e aumentar o DataSize, que irá aumentar a quantidade de séries temporais no dataset. O\n",
    "tamanho máximo que eu consegui testar no meu notebook sem ele morrer foi 150000 amostras.\n",
    "'''\n",
    "\n",
    "\n",
    "# Dataset size\n",
    "DataSize = 10\n",
    "\n",
    "# Serie size\n",
    "SerieSize = 200\n",
    "\n",
    "# Measurement resolution\n",
    "resolution = 3\n",
    "\n",
    "freqs = []\n",
    "fft_data = []\n",
    "pdm_data = []\n",
    "total_interp = 0\n",
    "\n",
    "# Calls necessary functions to create the dataset\n",
    "for i in range(DataSize):\n",
    "    amplitude = np.round(np.random.uniform(0.5, 1.5), 2)\n",
    "    w0 = np.round(np.random.uniform(1, 9), 2)\n",
    "    \n",
    "    x, y = sinusoidal(amplitude, w0, SerieSize, resolution)\n",
    "    \n",
    "    interp_start = time.time()\n",
    "    fft_data.append(interpolation(x, y, resolution))\n",
    "    interp_end = time.time()\n",
    "    total_interp+= (interp_end-interp_start)\n",
    "    \n",
    "    pdm_data.append((x, y))\n",
    "    freqs.append(w0)\n",
    "\n",
    "fft_data = pd.DataFrame(fft_data).T\n",
    "\n",
    "print(f\"Tempo gasto interpolando a função: {total_interp} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft, fftfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourierTransform(interpData):\n",
    "    \"\"\" calculates the points of the function in the frequency domain\n",
    "\n",
    "    Args:\n",
    "        interpolationData (dataframe): interpolated y points\n",
    "\n",
    "    Returns:\n",
    "        list: list of tulples containing the x and y points of the function in the frequency domain\n",
    "    \"\"\"\n",
    "    \n",
    "    fourierSeries = []\n",
    "    T = 1/10**resolution\n",
    "\n",
    "    # loop to cycle through all time series in the dataset\n",
    "    for i in interpData:\n",
    "        \n",
    "        # Remove NaN values from the dataframe\n",
    "        y_interpolation = interpData[i].dropna().values\n",
    "        \n",
    "        N = len(y_interpolation)\n",
    "        \n",
    "        # Here we apply the Forier transformation\n",
    "        yf = fft(y_interpolation)[:int(N/2)]\n",
    "        xf = fftfreq(N, T)[:int(N/2)]\n",
    "        \n",
    "        fourierSeries.append((xf,yf))\n",
    "        \n",
    "    return fourierSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcFreq():\n",
    "    \"\"\" Calculates the peak frequency and the period of the system\n",
    "\n",
    "    Returns:\n",
    "        list: frequency and period calculateds \n",
    "    \"\"\"\n",
    "    \n",
    "    transf_data = fourierTransform(fft_data)\n",
    "    fft_results = []\n",
    "    \n",
    "    # loop to cycle through all time series in the dataset\n",
    "    for i in range(len(transf_data)):\n",
    "        \n",
    "        xf = transf_data[i][0]\n",
    "        yf = transf_data[i][1]\n",
    "        \n",
    "        # calculates the peak frequency\n",
    "        best_frequencia = xf[np.absolute(yf).argmax()]\n",
    "        \n",
    "        # calculates the period\n",
    "        best_periodo = np.round(1/best_frequencia, resolution)\n",
    "\n",
    "        fft_results.append((best_frequencia, best_periodo))\n",
    "    \n",
    "    return fft_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Em alguns momentos, por algum motivo, a transformada de fourier não está funcionando. Acredito que seja algum problema relacionado à interpolação dos dados, mas\n",
    "como isso só ocorreu em algumas raras ocasiões, acabei deixando de lado e tentando focar nos algoritmos de machine learning. Mas fica o aviso, caso veja um warning\n",
    "sobre uma divisão por zero, é por isso.\n",
    "\"\"\"\n",
    "\n",
    "fft_start = time.time()\n",
    "\n",
    "fft_results = calcFreq()\n",
    "\n",
    "# loop to cycle through all fft results\n",
    "for i in range(len(fft_results)):\n",
    "    \n",
    "    # calculating frequency\n",
    "    w0 = np.round(2*np.pi*fft_results[i][0], resolution)\n",
    "    expected_w0 = freqs[i]\n",
    "    w0_error =  abs(np.round(((w0 - expected_w0)/expected_w0)*100, resolution))\n",
    "    \n",
    "    # calculating period\n",
    "    period = np.round(2*np.pi / w0, resolution)\n",
    "    expected_p = np.round(2*np.pi / expected_w0,resolution)\n",
    "    p_error =  abs(np.round(((period - expected_p)/expected_p)*100, resolution))\n",
    "    \n",
    "    # printing results\n",
    "    print(f\"w calculated: {w0}, expected w: {expected_w0} and error: {w0_error}% \")\n",
    "    print(f\"Period calculated: {period}, expected period: {expected_p} and error: {p_error}% \\n\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "fft_end = time.time()\n",
    "fft_spent = fft_end-fft_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDM - Phase Dispersion Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyAstronomy.pyTiming import pyPDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pdm(pdm_data):\n",
    "    \"\"\" Use the PDM method to calculate the system period\n",
    "\n",
    "    Args:\n",
    "        pdm_data (list): original sinusoidal function without interpolation\n",
    "    \"\"\"\n",
    "    \n",
    "    n_bins = 10\n",
    "    pdm_freqs = []\n",
    "    \n",
    "    # loop to cycle through all time series in the dataset\n",
    "    for i in range(len(pdm_data)):\n",
    "        \n",
    "        # implements PDM method\n",
    "        P = pyPDM.PyPDM(pdm_data[i][0], pdm_data[i][1])\n",
    "        scan = pyPDM.Scanner(minVal=1/10**resolution, maxVal=int(SerieSize/3), \n",
    "                            dVal=1/10**resolution, mode=\"frequency\")\n",
    "\n",
    "        # calculates the frequency and theta\n",
    "        frequencia, theta = P.pdmEquiBinCover(n_bins, 5, scan)\n",
    "        \n",
    "        # append the best frequency\n",
    "        pdm_freqs.append((2*np.pi*frequencia[theta.argmin()]))\n",
    "    \n",
    "    return pdm_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdm_start = time.time()\n",
    "\n",
    "pdm_results = calc_pdm(pdm_data)\n",
    "\n",
    "# loop to cycle through all pdm results\n",
    "for i in range(len(pdm_results)):\n",
    "    \n",
    "    # calculating frequency\n",
    "    w0 = np.round(pdm_results[i], resolution)\n",
    "    expected_w0 = freqs[i]\n",
    "    w0_error =  abs(np.round(((w0 - expected_w0)/expected_w0)*100, resolution))\n",
    "    \n",
    "    # calculating period\n",
    "    period = np.round(2*np.pi / w0, resolution)\n",
    "    expected_p = np.round(2*np.pi / expected_w0 ,resolution)\n",
    "    p_error =  abs(np.round(((period - expected_p)/expected_p)*100, resolution))\n",
    "    \n",
    "    # printing results\n",
    "    print(f\"w calculated: {w0}, expected w: {expected_w0} and error: {w0_error}% \")\n",
    "    print(f\"Period calculated: {period}, expected period: {expected_p} and error: {p_error}% \\n\")\n",
    "    \n",
    "pdm_end = time.time()\n",
    "pdm_spent = pdm_end-pdm_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliando o desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total time spent in method one\n",
    "total_fft = total_interp + fft_spent\n",
    "print(\"Method one: Interpolation + Fourier transform\")\n",
    "print(f\"Total time spent: {total_fft} s\\n\")\n",
    "\n",
    "# Total time spent in method two\n",
    "total_pdm= pdm_spent\n",
    "print(\"Method two: PDM\")\n",
    "print(f\"Total time spent: {total_pdm} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bom, se tudo ocorreu bem, você irá perceber que o tempo total gasto para interpolar a função e depois calcular a transformada de Fourier é muitas vezes mais rápido do que calcular pelo método PDM. Vale destacar também que o erro encontrado no método 1 é bem pequeno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de regressão (Extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bom, aqui segue minha falha tentativa de implementar um algoritmo de regressão. O melhor resultados que eu obtide, executando o LinearRegression, foi:\n",
    "\n",
    "Erro Médio Quadrado (MSE): 3.813668498412939\n",
    "Erro Absoluto Médio (MAE): 1.6037934491453092\n",
    "Coeficiente de Determinação (R-squared): 0.28300592960334636\n",
    "\n",
    "Para obter esse resultado, utilizei 150000 amostras, diminuindo a resolução para uma casa decimal. Acredito que aumentando mais o número de amostras, o resultado deva melhorar. Infelizmente, meu notebook simplesmente mata o processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input time series\n",
    "input_data = fft_data.fillna(0)\n",
    "\n",
    "# Corresponding output float values\n",
    "output_values = freqs\n",
    "\n",
    "# Convert lists into a numpy array\n",
    "X = np.array(input_data.T)\n",
    "y = np.array(output_values)\n",
    "\n",
    "# Scale the training data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Coefficient of Determination (R-squared): {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse modelo, ao contrário do primeiro, aplica uma regressão não linear. O problema é que ele demora mais para ser treinado. Deixei rodando por quase duas horas, com uma série reduzida, e mesmo assim ele não terminou de ser executado. Na teoria, ele deveria apresentar um resultado melhor que o anterior. Caso consiga rodar ele na sua máquina, me avise sobre os resultados, eu fiquei curioso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input time series\n",
    "input_data = fft_data.fillna(0)\n",
    "\n",
    "# Corresponding output float values\n",
    "output_values = freqs\n",
    "\n",
    "# Convert the time series into a numpy array\n",
    "X = np.array(input_data.T)\n",
    "\n",
    "# The model's target outputs\n",
    "y = np.array(output_values)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create and train the SVR model\n",
    "model_svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)  # 'rbf' is the radial basis function (Gaussian) kernel\n",
    "model_svr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_svr = model_svr.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics for the SVR model\n",
    "mse_svr = mean_squared_error(y_test, predictions_svr)\n",
    "mae_svr = mean_absolute_error(y_test, predictions_svr)\n",
    "r2_svr = r2_score(y_test, predictions_svr)\n",
    "\n",
    "# Print the evaluation metrics for the SVR model\n",
    "print(\"Evaluation Metrics for the SVR Model:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_svr}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_svr}\")\n",
    "print(f\"Coefficient of Determination (R-squared): {r2_svr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora não tenha sido possível obter resultados satisfatórios utilizando os algoritmos de regressão, o uso de interpolação somado à transformada de fourier parece ser promissor e mostrou reduzir significativamente o tempo de execução quando comparado ao PDM.\n",
    "\n",
    "Como possíveis incrementos desse projeto, destaca-se o treinamento dos algoritmos usados acima com um dataset mais robusto para identificar os seus reais potenciais. O uso de outros algoritmos de aprendizado, como algoritmos de redes neurais, também parece ser promissor para o caso. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
